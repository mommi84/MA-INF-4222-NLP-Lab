{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Convolution1D, MaxPooling1D, Dropout, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "\n",
    "'''\n",
    "Task 3: playing with NN framwork/keras and basic sentiment analysis\n",
    "- use the following model as a baseline and improve it!\n",
    "- export your metadata (just basic hyperparameters and outcomes for test data!)\n",
    "- test data = 0.3 (not in this example, change it!)\n",
    "- random_state = 4222\n",
    "- no need to cross-validation!\n",
    "'''\n",
    "\n",
    "#Parameters\n",
    "max_features = 500\n",
    "batch_size = 16\n",
    "epochs=2\n",
    "verbose= 2\n",
    "epochs=2\n",
    "validation_size = 1000\n",
    "random_state = 4222\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and transform data\n",
    "def get_df():\n",
    "    df = pd.read_csv('dataset_sentiment.csv') \n",
    "    df = df[['text','sentiment']]  #alternatively ,usecols=['text','sentiment'] above\n",
    "    #Transform\n",
    "    df = df[df.sentiment != \"Neutral\"]\n",
    "    df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "    df['text'] = df['text'].apply(lambda x: x.replace('rt',' '))\n",
    "    df['text'] = df['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "    return df\n",
    "def get_TrainTestData(df, max_features=500):\n",
    "    tok = Tokenizer(num_words=max_features, split=' ')\n",
    "    tok.fit_on_texts(df['text'].values)\n",
    "    # Take the texts and transform this data into numbers\n",
    "    X = tok.texts_to_sequences(df['text'].values)\n",
    "    X = pad_sequences(X)\n",
    "    # Take 'Sentiments' as teacher data\n",
    "    Y = pd.get_dummies(df['sentiment']).values\n",
    "    #Split data\n",
    "    print('X.shape[1]: ', X.shape[1], 'X.shape: ', X.shape)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\n",
    "    \n",
    "    #X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "    #X2 = tok.texts_to_sequences(X2)\n",
    "    #X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test, X.shape[1]#, X2\n",
    "def eval_ScoreAccuracy(X_test, Y_test, nn):  #, X2\n",
    "    print('evaluate Score and Accuracy on X_test')\n",
    "    X_validate = X_test[-validation_size:]\n",
    "    Y_validate = Y_test[-validation_size:]\n",
    "    X_test = X_test[:-validation_size]\n",
    "    Y_test = Y_test[:-validation_size]\n",
    "    score, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "    print(\"score: %.2f\" % (score))\n",
    "    print(\"acc: %.2f\" % (accuracy))\n",
    "    \n",
    "    print('evaluate Positive and Negative Accuracy..')\n",
    "    pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "    for x in range(len(X_validate)):\n",
    "        result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "        if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "            if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n",
    "            else: pos_ok += 1\n",
    "        if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n",
    "        else: pos_cnt += 1\n",
    "\n",
    "    print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "    print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n",
    "    #print('Predict on X2 now:')\n",
    "    #print(X2)\n",
    "    #print(nn.predict(X2, batch_size=1, verbose = 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Neural Network model\n",
    "def get_Neural_Network_Model_default(X_shape_1):\n",
    "    # Parameters\n",
    "    max_features = 500\n",
    "    embed_dim = 128\n",
    "    lstm_out = 196\n",
    "    dropout = 0.1\n",
    "    dropout_1d = 0.4\n",
    "    recurrent_dropout = 0.1\n",
    "    validation_size = 1000\n",
    "    batch_size = 16\n",
    "    epochs=2\n",
    "    verbose= 2\n",
    "    nn = Sequential()\n",
    "    nn.add(Embedding(max_features, embed_dim, input_length = X_shape_1))\n",
    "    nn.add(SpatialDropout1D(dropout_1d))\n",
    "    nn.add(LSTM(lstm_out, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "    nn.add(Dense(2, activation='softmax'))\n",
    "    nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(nn.summary())\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Neural_Network_Model_Improved(X_shape_1):\n",
    "    # Parameters\n",
    "    max_features = 500\n",
    "    embed_dim = 128\n",
    "\n",
    "    num_filters = 80\n",
    "    kernel_size = 5\n",
    "    pool_size = 3\n",
    "    lstm_out = 196\n",
    "    dropout = 0.1\n",
    "    recurrent_dropout = 0.1\n",
    "    #validation_size = 1000\n",
    "    #batch_size = 16\n",
    "    #epochs=2\n",
    "    #verbose= 2\n",
    "    \n",
    "    \n",
    "    nn = Sequential() \n",
    "    nn.add(Embedding(max_features, embed_dim, input_length = X_shape_1))\n",
    "    nn.add(Dropout(dropout))\n",
    "    nn.add(Convolution1D(filters=num_filters,kernel_size=kernel_size, padding='valid',activation = 'relu', strides=1))\n",
    "    nn.add(MaxPooling1D(pool_size=pool_size))\n",
    "    nn.add(LSTM(lstm_out, recurrent_dropout=recurrent_dropout))\n",
    "    nn.add(Dense(2, activation='softmax'))\n",
    "    nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(nn.summary())\n",
    "    \n",
    "    \n",
    "    # Add max pooling\n",
    "    #model.add(MaxPooling1D(pool_size=pool_size))\n",
    "\n",
    "    # Add LSTM\n",
    "    #model.add(LSTM(lstm_output_size))\n",
    "\n",
    "    # Add output\n",
    "    #model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see also https://blog.keras.io/category/tutorials.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape[1]:  26 X.shape:  (10729, 26)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 319,194\n",
      "Trainable params: 319,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 36s - loss: 0.4324 - acc: 0.8204\n",
      "Epoch 2/2\n",
      " - 33s - loss: 0.3670 - acc: 0.8463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2314e3b29b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, X_shape_1 = get_TrainTestData(get_df(), max_features=500)\n",
    "nn = get_Neural_Network_Model_default(X_shape_1)\n",
    "nn.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate Score and Accuracy on X_test\n",
      "score: 0.36\n",
      "acc: 0.85\n",
      "evaluate Positive and Negative Accuracy..\n",
      "pos_acc 32.19512195121951 %\n",
      "neg_acc 97.61006289308176 %\n"
     ]
    }
   ],
   "source": [
    "eval_ScoreAccuracy(X_test, y_test, nn) #, X2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 22, 80)            51280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 7, 80)             0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 196)               217168    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 332,842\n",
      "Trainable params: 332,842\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 23s - loss: 0.4211 - acc: 0.8212\n",
      "Epoch 2/2\n",
      " - 21s - loss: 0.3435 - acc: 0.8563\n",
      "evaluate Score and Accuracy on X_test\n",
      "score: 0.38\n",
      "acc: 0.83\n",
      "evaluate Positive and Negative Accuracy..\n",
      "pos_acc 54.146341463414636 %\n",
      "neg_acc 91.19496855345912 %\n"
     ]
    }
   ],
   "source": [
    "nn2 = get_Neural_Network_Model_Improved(X_shape_1)\n",
    "nn2.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n",
    "eval_ScoreAccuracy(X_test, y_test, nn2) #,X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Alternatively could use for Neural Network model:\n",
    "# #see https://elitedatascience.com/keras-tutorial-deep-learning-in-python\n",
    "# def get_Neural_Network_Model_ddddd(X_shape_1):\n",
    "#     # Parameters\n",
    "#     max_features = 500\n",
    "#     embed_dim = 128\n",
    "    \n",
    "#     num_filters = 100\n",
    "#     kernel_size = 3\n",
    "#     dropout = 0.1\n",
    "\n",
    "#     nn = Sequential()    \n",
    "#     nn.add(Embedding(max_features, embed_dim, input_length = X_shape_1)) \n",
    "#     #nn.add(Convolution1D(nb_filter=32, filter_length=8, border_mode='valid',input_dim=128))  \n",
    "\n",
    "    \n",
    "    \n",
    "#     nn.add(Convolution1D(filters=num_filters,kernel_size=kernel_size, padding='valid',activation = 'relu', strides=1))\n",
    "#     nn.add(MaxPooling1D(pool_size=5))\n",
    "#     nn.add(Dropout(0.25)) \n",
    "#     nn.add(Flatten())\n",
    "#     nn.add(Dense(128, activation='relu'))\n",
    "#     nn.add(Dropout(dropout))\n",
    "#     nn.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "#     nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "#     print(nn.summary())\n",
    "    \n",
    "#     return nn\n",
    "\n",
    "# #    nn.add(Embedding(max_features, embed_dim, input_length = sequence_length))\n",
    "# #    nn.add(Convolution1D(filters=num_filters, kernel_size=kernel_size, padding=\"valid\", activation=\"relu\", strides=1))\n",
    "# #    nn.add(MaxPooling1D(pool_size=2))\n",
    "# #    nn.add(Flatten())\n",
    "# #    nn.add(Dense(2, activation='softmax'))\n",
    "# #    nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nn3 = get_Neural_Network_Model_ddddd(X_shape_1)\n",
    "#nn3.fit(X_train, y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n",
    "#eval_ScoreAccuracy(X_test, y_test, nn2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

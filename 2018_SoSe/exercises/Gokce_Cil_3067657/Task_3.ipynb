{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D,MaxPooling1D,Conv1D,Flatten\n",
    "from keras.layers.recurrent import GRU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text sentiment\n",
      "0  RT @NancyLeeGrahn: How did everyone feel about...   Neutral\n",
      "1  RT @ScottWalker: Didn't catch the full #GOPdeb...  Positive\n",
      "2  RT @TJMShow: No mention of Tamir Rice and the ...   Neutral\n",
      "3  RT @RobGeorge: That Carly Fiorina is trending ...  Positive\n",
      "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...  Positive\n",
      "5  RT @GregAbbott_TX: @TedCruz: \"On my first day ...  Positive\n",
      "6  RT @warriorwoman91: I liked her and was happy ...  Negative\n",
      "7  Going on #MSNBC Live with @ThomasARoberts arou...   Neutral\n",
      "8  Deer in the headlights RT @lizzwinstead: Ben C...  Negative\n",
      "9  RT @NancyOsborne180: Last night's debate prove...  Negative\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 319,194\n",
      "Trainable params: 319,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 41s - loss: 0.4349 - acc: 0.8168\n",
      "Epoch 2/2\n",
      " - 42s - loss: 0.3629 - acc: 0.8478\n",
      "score: 0.37\n",
      "acc: 0.84\n",
      "pos_acc 30.208333333333332 %\n",
      "neg_acc 97.89603960396039 %\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 26, 32)            15456     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 26, 16)            2352      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 8)                 600       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 82,426\n",
      "Trainable params: 82,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 72s - loss: 0.4244 - acc: 0.8237\n",
      "Epoch 2/2\n",
      " - 72s - loss: 0.3637 - acc: 0.8461\n",
      "score: 0.36\n",
      "acc: 0.85\n",
      "pos_acc 44.87804878048781 %\n",
      "neg_acc 95.9748427672956 %\n",
      "['what are u going to say about that? the truth, wassock?!']\n",
      "lelele\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37\n",
      "  311 189   4 144  22  16   1 281]]\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 26, 32)            12320     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 13, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 416)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 250)               104250    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 502       \n",
      "=================================================================\n",
      "Total params: 181,072\n",
      "Trainable params: 181,072\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " - 3s - loss: 0.4228 - acc: 0.8168\n",
      "Epoch 2/2\n",
      " - 2s - loss: 0.3111 - acc: 0.8679\n",
      "score: 0.37\n",
      "acc: 0.85\n",
      "pos_acc 36.09756097560975 %\n",
      "neg_acc 96.60377358490567 %\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37 311\n",
      "  189   4 144  22  16   1 281]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected embedding_4_input to have shape (26,) but got array with shape (25,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-8ff8e531c13c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewModel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1815\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1816\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1817\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    121\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    124\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected embedding_4_input to have shape (26,) but got array with shape (25,)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "Task 3: playing with NN framwork/keras and basic sentiment analysis\n",
    "- use the following model as a baseline and improve it!\n",
    "- export your metadata (just basic hyperparameters and outcomes for test data!)\n",
    "- test data = 0.3 (not in this example, change it!)\n",
    "- random_state = 4222\n",
    "- no need to cross-validation!\n",
    "'''\n",
    "\n",
    "# parameters\n",
    "max_fatures = 500\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "dropout = 0.1\n",
    "dropout_1d = 0.4\n",
    "recurrent_dropout = 0.1\n",
    "random_state = 1324\n",
    "validation_size = 1000\n",
    "#validation_size = 200\n",
    "batch_size = 16\n",
    "epochs=2\n",
    "verbose= 2\n",
    "test_s = 0.3\n",
    "\n",
    "df = pd.read_csv('dataset_sentiment.csv')\n",
    "df = df[['text','sentiment']]\n",
    "print(df[0:10])\n",
    "\n",
    "df = df[df.sentiment != \"Neutral\"]\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('rt',' '))\n",
    "df['text'] = df['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    " \n",
    "#df = df.sample(2000)\n",
    "    \n",
    "tok = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tok.fit_on_texts(df['text'].values)\n",
    "X = tok.texts_to_sequences(df['text'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "nn = Sequential()\n",
    "nn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "nn.add(SpatialDropout1D(dropout_1d))\n",
    "nn.add(LSTM(lstm_out, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "nn.add(Dense(2, activation='softmax'))\n",
    "\n",
    "nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(nn.summary())\n",
    "\n",
    "Y = pd.get_dummies(df['sentiment']).values\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = test_s, random_state = random_state)\n",
    "nn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "\n",
    "score, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accuracy))\n",
    "\n",
    "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n",
    "        else: pos_ok += 1\n",
    "    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n",
    "    else: pos_cnt += 1\n",
    "\n",
    "print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n",
    "\n",
    "#for new model 1\n",
    "X_train_n1, X_test_n1, Y_train_n1, Y_test_n1 = train_test_split(X, Y, test_size = 0.3, random_state = 4222)\n",
    "X_validate_n1 = X_test_n1[-validation_size:]\n",
    "Y_validate_n1 = Y_test_n1[-validation_size:]\n",
    "X_test_n1 = X_test_n1[:-validation_size]\n",
    "Y_test_n1 = Y_test_n1[:-validation_size]\n",
    "#end\n",
    "#config = nn.get_config()\n",
    "#newModel = Sequential.from_config(config)\n",
    "\n",
    "newModel1 = Sequential()\n",
    "newModel1.add(Embedding(max_fatures, 128, input_length = X.shape[1]))\n",
    "newModel1.add(GRU(units=32, name = \"gru_1\",return_sequences=True))\n",
    "newModel1.add(GRU(units=16, name = \"gru_2\",return_sequences=True ))\n",
    "newModel1.add(GRU(units=8, name= \"gru_3\"))\n",
    "newModel1.add(Dense(2, activation='sigmoid'))\n",
    "newModel1.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "print(newModel1.summary())\n",
    "newModel1.fit(X_train_n1, Y_train_n1, epochs = epochs, batch_size=16, verbose=verbose)\n",
    "\n",
    "score, accuracy = 0,0\n",
    "score, accuracy = newModel1.evaluate(X_test_n1, Y_test_n1, verbose = 2, batch_size = 16)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accuracy))\n",
    "\n",
    "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "for x in range(len(X_validate_n1)):\n",
    "    result = newModel1.predict(X_validate_n1[x].reshape(1,X_test_n1.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_validate_n1[x]):\n",
    "        if np.argmax(Y_validate_n1[x]) == 0: neg_ok += 1\n",
    "        else: pos_ok += 1\n",
    "    if np.argmax(Y_validate_n1[x]) == 0: neg_cnt += 1\n",
    "    else: pos_cnt += 1\n",
    "print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n",
    "\n",
    "X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "print(X2)\n",
    "print('lelele')\n",
    "X2 = tok.texts_to_sequences(X2)\n",
    "X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "print(X2)\n",
    "\n",
    "\n",
    "#for new model 2\n",
    "X_train_n2, X_test_n2, Y_train_n2, Y_test_n2 = train_test_split(X, Y, test_size = 0.3, random_state = 4222)\n",
    "X_validate_n2 = X_test_n2[-validation_size:]\n",
    "Y_validate_n2 = Y_test_n2[-validation_size:]\n",
    "X_test_n2 = X_test_n2[:-validation_size]\n",
    "Y_test_n2 = Y_test_n2[:-validation_size]\n",
    "\n",
    "newModel2 = Sequential()\n",
    "newModel2.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "newModel2.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "newModel2.add(MaxPooling1D(pool_size=2))\n",
    "newModel2.add(Flatten())\n",
    "newModel2.add(Dense(250, activation='relu'))\n",
    "newModel2.add(Dense(2, activation='softmax'))\n",
    "newModel2.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "\n",
    "print(newModel2.summary())\n",
    "newModel2.fit(X_train_n2, Y_train_n2, epochs = epochs, batch_size=16, verbose=verbose)\n",
    "\n",
    "score, accuracy = 0,0\n",
    "score, accuracy = newModel2.evaluate(X_test_n2, Y_test_n2, verbose = 2, batch_size = 16)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accuracy))\n",
    "\n",
    "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "for x in range(len(X_validate_n2)):\n",
    "    result = newModel2.predict(X_validate_n2[x].reshape(1,X_test_n2.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_validate_n2[x]):\n",
    "        if np.argmax(Y_validate_n2[x]) == 0: neg_ok += 1\n",
    "        else: pos_ok += 1\n",
    "    if np.argmax(Y_validate_n2[x]) == 0: neg_cnt += 1\n",
    "    else: pos_cnt += 1\n",
    "print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")\n",
    "\n",
    "X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "X2 = tok.texts_to_sequences(X2)\n",
    "X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "print(X2)\n",
    "\n",
    "\n",
    "#print(nn.predict(X2, batch_size=1, verbose = 2)[0])\n",
    "#print(newModel1.predict(X2, batch_size=1, verbose = 2)[0])\n",
    "#print(newModel2.predict(X2, batch_size=1, verbose = 2)[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

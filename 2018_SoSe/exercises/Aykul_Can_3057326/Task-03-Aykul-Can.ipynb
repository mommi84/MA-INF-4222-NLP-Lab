{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTask 3: playing with NN framwork/keras and basic sentiment analysis\\n- use the following model as a baseline and improve it!\\n- export your metadata (just basic hyperparameters and outcomes for test data!)\\n- test data = 0.3 (not in this example, change it!)\\n- random_state = 4222\\n- no need to cross-validation!\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Task 3: playing with NN framwork/keras and basic sentiment analysis\n",
    "- use the following model as a baseline and improve it!\n",
    "- export your metadata (just basic hyperparameters and outcomes for test data!)\n",
    "- test data = 0.3 (not in this example, change it!)\n",
    "- random_state = 4222\n",
    "- no need to cross-validation!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Dropout, GRU, Convolution1D, GlobalMaxPooling1D, Activation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "max_fatures = 500\n",
    "embed_dim = 128\n",
    "lstm_out = 196\n",
    "dropout = 0.1\n",
    "dropout_1d = 0.4\n",
    "recurrent_dropout = 0.1\n",
    "random_state = 1324\n",
    "validation_size = 1000\n",
    "batch_size = 16\n",
    "epochs=2\n",
    "verbose= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text sentiment\n",
      "1     scottwalker didnt catch the full gopdebate l...  Positive\n",
      "3     robgeorge that carly fiorina is trending  ho...  Positive\n",
      "4     danscavino gopdebate w realdonaldtrump deliv...  Positive\n",
      "5     gregabbott_tx tedcruz on my first day i will...  Positive\n",
      "6     warriorwoman91 i liked her and was happy whe...  Negative\n",
      "8   deer in the headlights   lizzwinstead ben cars...  Negative\n",
      "9     nancyosborne180 last nights debate proved it...  Negative\n",
      "10  jgreendc realdonaldtrump in all fairness billc...  Negative\n",
      "11    waynedupreeshow just woke up to tweet this o...  Positive\n",
      "12  me reading my familys comments about how great...  Negative\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n",
      "[[363, 122, 1, 2, 39, 58, 237, 37, 210, 6, 174, 12]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0 363 122   1   2\n",
      "   39  58 237  37 210   6 174  12]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('dataset_sentiment.csv')\n",
    "df = df[['text','sentiment']]\n",
    "\n",
    "\n",
    "df = df[df.sentiment != \"Neutral\"]\n",
    "df['text'] = df['text'].apply(lambda x: x.lower())\n",
    "df['text'] = df['text'].apply(lambda x: x.replace('rt',' '))\n",
    "df['text'] = df['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "print(df[0:10])\n",
    "\n",
    "Y = pd.get_dummies(df['sentiment']).values\n",
    "print(Y[0:10])\n",
    "\n",
    "tok = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tok.fit_on_texts(df['text'].values)\n",
    "X = tok.texts_to_sequences(df['text'].values)\n",
    "print(X[0:1])\n",
    "X = pad_sequences(X)\n",
    "print(X[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model (Please comment out necessary models while testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 25, 128)           32896     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 105,282\n",
      "Trainable params: 105,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/egebuyuksemerci/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "/Users/egebuyuksemerci/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"relu\", filters=128, kernel_size=2, strides=1, padding=\"valid\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1], dropout=0.2))\n",
    "nn.add(\n",
    "    Convolution1D(\n",
    "        nb_filter=128,\n",
    "        filter_length=2,\n",
    "        border_mode='valid',\n",
    "        activation='relu',\n",
    "        subsample_length=1,\n",
    "    )\n",
    ")\n",
    "nn.add(GlobalMaxPooling1D())\n",
    "nn.add(Dense(64))\n",
    "nn.add(Dropout(0.2))\n",
    "nn.add(Activation('relu'))\n",
    "nn.add(Dense(2))\n",
    "nn.add(Activation('softmax'))\n",
    "nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 3s - loss: 0.4145 - acc: 0.8273\n",
      "Epoch 2/2\n",
      " - 3s - loss: 0.3250 - acc: 0.8627\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x105dc3208>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\n",
    "nn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.36\n",
      "acc: 0.84\n"
     ]
    }
   ],
   "source": [
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing results for model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 50.0 %\n",
      "neg_acc 94.05940594059405 %\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n",
    "        else: pos_ok += 1\n",
    "    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n",
    "    else: pos_cnt += 1\n",
    "\n",
    "print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing model 1 with a random sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37\n",
      "  311 189   4 144  22  16   1 281]]\n",
      "[ 0.96855766  0.0314423 ]\n"
     ]
    }
   ],
   "source": [
    "X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "X2 = tok.texts_to_sequences(X2)\n",
    "X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "print(X2)\n",
    "print(nn.predict(X2, batch_size=1, verbose = 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 26, 32)            15456     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 26, 16)            2352      \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 8)                 600       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 18        \n",
      "=================================================================\n",
      "Total params: 82,426\n",
      "Trainable params: 82,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "\n",
    "nn.add(GRU(units=32, name = \"gru_1\",return_sequences=True))\n",
    "nn.add(GRU(units=16, name = \"gru_2\" ,return_sequences=True))\n",
    "nn.add(GRU(units=8, name= \"gru_3\"))\n",
    "nn.add(Dense(2, activation='sigmoid',name=\"dense_1\"))\n",
    "nn.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 74s - loss: 0.4350 - acc: 0.8274\n",
      "Epoch 2/2\n",
      " - 81s - loss: 0.3650 - acc: 0.8538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a142c4048>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\n",
    "nn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.38\n",
      "acc: 0.84\n"
     ]
    }
   ],
   "source": [
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing results for model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 45.83333333333333 %\n",
      "neg_acc 94.80198019801979 %\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n",
    "        else: pos_ok += 1\n",
    "    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n",
    "    else: pos_cnt += 1\n",
    "\n",
    "print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing model 2 with a random sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37\n",
      "  311 189   4 144  22  16   1 281]]\n",
      "[ 0.89606792  0.08673313]\n"
     ]
    }
   ],
   "source": [
    "X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "X2 = tok.texts_to_sequences(X2)\n",
    "X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "print(X2)\n",
    "print(nn.predict(X2, batch_size=1, verbose = 2)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 26, 128)           64000     \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 26, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 196)               254800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 394       \n",
      "=================================================================\n",
      "Total params: 319,194\n",
      "Trainable params: 319,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nn = Sequential()\n",
    "nn.add(Embedding(max_fatures, embed_dim, input_length = X.shape[1]))\n",
    "nn.add(SpatialDropout1D(dropout_1d))\n",
    "nn.add(LSTM(lstm_out, dropout=dropout, recurrent_dropout=recurrent_dropout))\n",
    "nn.add(Dense(2, activation='softmax'))\n",
    "nn.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(nn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      " - 55s - loss: 0.4341 - acc: 0.8172\n",
      "Epoch 2/2\n",
      " - 51s - loss: 0.3606 - acc: 0.8498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1e2044e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.30, random_state = random_state)\n",
    "nn.fit(X_train, Y_train, epochs = epochs, batch_size=batch_size, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.37\n",
      "acc: 0.84\n"
     ]
    }
   ],
   "source": [
    "X_validate = X_test[-validation_size:]\n",
    "Y_validate = Y_test[-validation_size:]\n",
    "X_test = X_test[:-validation_size]\n",
    "Y_test = Y_test[:-validation_size]\n",
    "score, accuracy = nn.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Printing results for model 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_acc 37.5 %\n",
      "neg_acc 95.42079207920791 %\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_ok, neg_ok = 0, 0, 0, 0\n",
    "for x in range(len(X_validate)):\n",
    "    result = nn.predict(X_validate[x].reshape(1,X_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    if np.argmax(result) == np.argmax(Y_validate[x]):\n",
    "        if np.argmax(Y_validate[x]) == 0: neg_ok += 1\n",
    "        else: pos_ok += 1\n",
    "    if np.argmax(Y_validate[x]) == 0: neg_cnt += 1\n",
    "    else: pos_cnt += 1\n",
    "\n",
    "print(\"pos_acc\", pos_ok/pos_cnt*100, \"%\")\n",
    "print(\"neg_acc\", neg_ok/neg_cnt*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing model 3 with a random sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48  37\n",
      "  311 189   4 144  22  16   1 281]]\n",
      "[ 0.88296747  0.11703251]\n"
     ]
    }
   ],
   "source": [
    "X2 = ['what are u going to say about that? the truth, wassock?!']\n",
    "X2 = tok.texts_to_sequences(X2)\n",
    "X2 = pad_sequences(X2, maxlen=26, dtype='int32', value=0)\n",
    "print(X2)\n",
    "print(nn.predict(X2, batch_size=1, verbose = 2)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

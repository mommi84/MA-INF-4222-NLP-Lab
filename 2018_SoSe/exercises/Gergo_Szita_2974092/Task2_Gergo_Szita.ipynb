{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./fake_or_real_news.csv')\n",
    "df = df.set_index('Unnamed: 0')\n",
    "\n",
    "y = df.label\n",
    "df = df.drop('label', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train)\n",
    "count_test = count_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    See full source and example: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(tfidf_train, y_train)\n",
    "pred = clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "scores = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision: %0.3f\" % scores[0])\n",
    "print(\"recall: %0.3f\" % scores[1])\n",
    "print(\"f-score: %0.3f\" % scores[2])\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "\n",
    "clf.fit(count_train, y_train)\n",
    "pred = clf.predict(count_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "scores = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision:   %0.3f\" % scores[0])\n",
    "print(\"recall:   %0.3f\" % scores[1])\n",
    "print(\"f-score:   %0.3f\" % scores[2])\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['FAKE', 'REAL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "\n",
    "lin_clf.fit(tfidf_train, y_train)\n",
    "pred = lin_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "scores = precision_recall_fscore_support(y_test, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision:   %0.3f\" % scores[0])\n",
    "print(\"recall:   %0.3f\" % scores[1])\n",
    "print(\"f-score:   %0.3f\" % scores[2])\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred, labels=['FAKE', 'REAL'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['FAKE', 'REAL'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('liar_dataset/train.tsv', sep=\"\\t\", header=None)\n",
    "\n",
    "X_train = df[2]\n",
    "y_train = df[1]\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('liar_dataset/test.tsv', sep=\"\\t\", header=None)\n",
    "\n",
    "X_test = df[2]\n",
    "y_test = df[1]\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing from multiple classification to binary\n",
    "def FromMulToBi(train_set):\n",
    "    y_train_sc_1 = train_set.copy()\n",
    "\n",
    "    for i in range(len(y_train_sc_1)):\n",
    "        if y_train[i] != 'true':\n",
    "            y_train_sc_1[i] = 'false'\n",
    "    \n",
    "    return y_train_sc_1\n",
    "\n",
    "y_train_sc1 = FromMulToBi(y_train)\n",
    "y_test_sc_1 = FromMulToBi(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "\n",
    "lin_clf.fit(tfidf_train, y_train_sc_1)\n",
    "pred = lin_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test_sc_1, pred)\n",
    "scores = precision_recall_fscore_support(y_test_sc_1, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision:   %0.3f\" % scores[0])\n",
    "print(\"recall:   %0.3f\" % scores[1])\n",
    "print(\"f-score:   %0.3f\" % scores[2])\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred, labels=['true', 'false'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['true', 'false'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = KNeighborsClassifier(3)\n",
    "\n",
    "lin_clf.fit(tfidf_train, y_train_sc_1)\n",
    "pred = lin_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test_sc_1, pred)\n",
    "scores = precision_recall_fscore_support(y_test_sc_1, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision:   %0.3f\" % scores[0])\n",
    "print(\"recall:   %0.3f\" % scores[1])\n",
    "print(\"f-score:   %0.3f\" % scores[2])\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred, labels=['true', 'false'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['true', 'false'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "lin_clf.fit(tfidf_train, y_train_sc_1)\n",
    "pred = lin_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(y_test_sc_1, pred)\n",
    "scores = precision_recall_fscore_support(y_test_sc_1, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision:   %0.3f\" % scores[0])\n",
    "print(\"recall:   %0.3f\" % scores[1])\n",
    "print(\"f-score:   %0.3f\" % scores[2])\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, pred, labels=['true', 'false'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['true', 'false'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "#ds1 = sys.argv[1]\n",
    "#ds2 = sys.argv[2]\n",
    "ds1 = './fake_or_real_news.csv'\n",
    "ds2 = 'liar_dataset/train.tsv'\n",
    "\n",
    "def get_dataset3_split(dataset1_in, dataset2_in):\n",
    "    try:\n",
    "        print('processing datasets')\n",
    "        print('ds1=', dataset1_in)\n",
    "        print('ds2=', dataset2_in)\n",
    "\n",
    "        print('-- fake news')\n",
    "        df1 = pd.read_csv(dataset1_in, sep=',', usecols=['title','text','label'])\n",
    "        df1['claim'] = df1[['title', 'text']].apply(lambda x: '. '.join(x), axis=1)\n",
    "        del df1['title']\n",
    "        del df1['text']\n",
    "        df1.rename(index=str, columns={'label': 'y'}, inplace=True)\n",
    "        print(df1.keys())\n",
    "        print(len(df1[df1['y']=='REAL']))\n",
    "        print(len(df1[df1['y']=='FAKE']))\n",
    "        df1['y'] = np.where(df1['y'] == 'FAKE', 'false', 'true')\n",
    "        print(len(df1))\n",
    "\n",
    "        print('-- liar liar')\n",
    "        df2 = pd.read_csv(dataset2_in, sep='\\t', header=None, usecols=[1,2], names=['y', 'claim'])\n",
    "        print(df2.keys())\n",
    "        print(set(df2.y), len(df2))\n",
    "        print(len(df2[df2['y'] == 'true']))\n",
    "        print(len(df2[df2['y'] == 'false']))\n",
    "        df2=df2[(df2['y'] == 'true') | (df2['y'] == 'false')]\n",
    "        print(set(df2.y), len(df2))\n",
    "\n",
    "        df3=pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "        print(df3['y'].value_counts())\n",
    "        print('done')\n",
    "        return train_test_split(df3['claim'], df3['y'], test_size=0.30, random_state=35)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "ds3_train, ds3_test, ds3_y_train, ds3_y_test = get_dataset3_split(ds1,ds2)\n",
    "print(len(ds3_y_train))\n",
    "print(len(ds3_train))\n",
    "print(len(ds3_test))\n",
    "print(ds3_y_train.get_values()[1], ds3_train.get_values()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(ds3_train)\n",
    "tfidf_test = tfidf_vectorizer.transform(ds3_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = PassiveAggressiveClassifier(n_iter=50)\n",
    "\n",
    "print(len(y_train_sc_1))\n",
    "\n",
    "lin_clf.fit(tfidf_train, ds3_y_train)\n",
    "pred = lin_clf.predict(tfidf_test)\n",
    "score = metrics.accuracy_score(ds3_y_test, pred)\n",
    "scores = precision_recall_fscore_support(ds3_y_test, pred, average='weighted')\n",
    "print(\"accuracy:   %0.3f\" % score)\n",
    "print(\"precision:   %0.3f\" % scores[0])\n",
    "print(\"recall:   %0.3f\" % scores[1])\n",
    "print(\"f-score:   %0.3f\" % scores[2])\n",
    "\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(ds3_test, pred, labels=['true', 'false'])\n",
    "plot_confusion_matrix(confusion_matrix, classes=['true', 'false'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python_3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
